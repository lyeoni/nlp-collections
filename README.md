# Natural Language Processing Collections
Collection of Natural Language Processing Models and References 

## Word Embeddings
- ****`CODE`**** [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
- ****`CODE`**** [FastText](https://github.com/facebookresearch/fastText)

## Attention & Transformer
- ****`PAPER`**** [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- ****`POST`**** [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- ****`POST`**** [Transformer Architecture: Attention Is All You Need](https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09)

## Language Modeling
- ****`PAPER`**** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- ****`VIDEO(Korean ver.)`**** [PR-121: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.youtube.com/watch?v=GK4IO3qOnLc)
- ****`VIDEO(Korean ver.)`**** [#BERT #충격적인논문 #ClovaAI](https://www.facebook.com/hunkims/videos/10156797151174521/)
- ****`VIDEO(Korean ver.)`**** [BERT 세미나 - TmaxData NLP 박민호](https://www.youtube.com/watch?v=2b7_iq8rAVY&t=367s)

## NLP Lectures
- ****`VIDEO`**** [CS224n: Natural Language Processing with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
- ****`VIDEO(Korean ver.)`**** [딥러닝을 이용한 자연어 처리 - 조경현 교수(NYU)](https://www.edwith.org/deepnlp/joinLectures/17363)
